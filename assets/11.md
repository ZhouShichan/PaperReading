
<table>
    <thead>
        <tr>
            <th>Model</th>
            <th>Parametername</th>
            <th>Parameterrange</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="3">TFT</td>
            <td>#head</td>
            <td>1,2, 4</td>
        </tr>
        <tr>
            <td>hidden size</td>
            <td>96,128, 192, 256</td>
        </tr>
        <tr>
            <td>dropout</td>
            <td>(0,0.5)</td>
        </tr>
        <tr>
            <td rowspan="17">N-BEATS</td>
            <td>stack0 type</td>
            <td>trend,generic</td>
        </tr>
        <tr>
            <td>stack1 type</td>
            <td>seasonality,generic</td>
        </tr>
        <tr>
            <td>stack0 # blocks</td>
            <td>2, 4,8</td>
        </tr>
        <tr>
            <td>stack1 # blocks</td>
            <td>2, 4,8</td>
        </tr>
        <tr>
            <td>stack0 theta dim</td>
            <td>2, 4,8, 16</td>
        </tr>
        <tr>
            <td>stack1 theta dim</td>
            <td>0, 2,4, 8, 16</td>
        </tr>
        <tr>
            <td>stack0 share weights</td>
            <td>True,False</td>
        </tr>
        <tr>
            <td>stack1 share weights</td>
            <td>True,False</td>
        </tr>
        <tr>
            <td>stack0 hidden size</td>
            <td>256,512, 1024, 2048, 4096*</td>
        </tr>
        <tr>
            <td>stack1 hidden size</td>
            <td>256,512, 1024, 2048, 4096*</td>
        </tr>
        <tr>
            <td>mlp layers</td>
            <td>2,3,4</td>
        </tr>
        <tr>
            <td>hidden size</td>
            <td>256,512,1024,2048</td>
        </tr>
        <tr>
            <td>activation</td>
            <td>ReLU,Softplus, Tanh, SELU, LeakyReLU, PReLU, Sigmoid</td>
        </tr>
        <tr>
            <td>pooling mode</td>
            <td>MaxPool1d,AvgPool1d</td>
        </tr>
        <tr>
            <td>#blocks</td>
            <td>[1,1,1],[1,1,2],[1,2,1],[1,2,2],[2,1,1],[2,1,2],[2,2,1],[2,2,2]</td>
        </tr>
        <tr>
            <td>poolkernel size</td>
            <td>[6,3,1],[6,2,1],[4,2,1],[3,3,1],[2,2,1]</td>
        </tr>
        <tr>
            <td>freq downsample</td>
            <td>[6,3,1],[6,2,1],[4,2,1],[3,3,1],[2,2,1]</td>
        </tr>
        <tr>
            <td rowspan="3">DeepAR</td>
            <td>#layers</td>
            <td>2,3,4,5</td>
        </tr>
        <tr>
            <td>hidden size</td>
            <td>64,128,256, 512, 1024, 2048**</td>
        </tr>
        <tr>
            <td>dropout</td>
            <td>(0,0.6)</td>
        </tr>
        <tr>
            <td rowspan="13">MTGNN</td>
            <td>gcn depth</td>
            <td>2,3,4</td>
        </tr>
        <tr>
            <td>dropout</td>
            <td>(0,0.5)</td>
        </tr>
        <tr>
            <td>subgraph size</td>
            <td>10,15,20</td>
        </tr>
        <tr>
            <td>node dim</td>
            <td>32,40,64,128</td>
        </tr>
        <tr>
            <td>conv channels</td>
            <td>16,32,64</td>
        </tr>
        <tr>
            <td>residual channels</td>
            <td>16,32,64</td>
        </tr>
        <tr>
            <td>skip channels</td>
            <td>32,64</td>
        </tr>
        <tr>
            <td>end channels</td>
            <td>32,64</td>
        </tr>
        <tr>
            <td>#layers</td>
            <td>2,3,4</td>
        </tr>
        <tr>
            <td>propalpha</td>
            <td>(0.01,0.2)</td>
        </tr>
        <tr>
            <td>tanalpha</td>
            <td>(2.0,4.0)</td>
        </tr>
        <tr>
            <td>in dim</td>
            <td>16,24, 32, 64</td>
        </tr>
        <tr>
            <td>embedding</td>
            <td>True,False</td>
        </tr>
        <tr>
            <td rowspan="4">DCRNN</td>
            <td>maxdiffusion step</td>
            <td>1,2</td>
        </tr>
        <tr>
            <td>#RNN layers</td>
            <td>2,3</td>
        </tr>
        <tr>
            <td>RNN units</td>
            <td>32,64, 128</td>
        </tr>
        <tr>
            <td>activation</td>
            <td>tanh,ReLU</td>
        </tr>
    </tbody>
</table>
